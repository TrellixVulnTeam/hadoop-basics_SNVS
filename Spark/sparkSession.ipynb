{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1c1dde02-3826-48e5-8b56-b9fb02379d5d",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pyspark'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Input \u001b[0;32mIn [3]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpyspark\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msql\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SparkSession\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'pyspark'"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a2dd6819-a10c-4469-abc1-76a5e1d7ab05",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e0671da0-2713-475a-addd-8f6c3f15d51a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/01/24 14:10:12 WARN Utils: Your hostname, Ayushmans-MacBook-Air.local resolves to a loopback address: 127.0.0.1; using 192.168.1.236 instead (on interface en0)\n",
      "22/01/24 14:10:12 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "22/01/24 14:10:12 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder.master(\"local[2]\").appName(\"Session 1\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "74aea2ab-d277-4f88-932f-05e192279ad7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reading csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "56ebe7f9-24ea-480c-99f1-9212e070bef2",
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_file = './data/basic.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "36679cf8-0973-4a96-b8f3-2ed391e8de05",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.csv(csv_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "702a8b0e-f12d-486e-b214-082aefeec828",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.csv('./data/stocks_price_final.csv', sep=',', header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "909724c7-941d-4f81-a1a7-9f1431e8f01a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _c0: string (nullable = true)\n",
      " |-- symbol: string (nullable = true)\n",
      " |-- date: string (nullable = true)\n",
      " |-- open: string (nullable = true)\n",
      " |-- high: string (nullable = true)\n",
      " |-- low: string (nullable = true)\n",
      " |-- close: string (nullable = true)\n",
      " |-- volume: string (nullable = true)\n",
      " |-- adjusted: string (nullable = true)\n",
      " |-- market.cap: string (nullable = true)\n",
      " |-- sector: string (nullable = true)\n",
      " |-- industry: string (nullable = true)\n",
      " |-- exchange: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "24c118b4-ab2a-4b17-92a7-605932c74609",
   "metadata": {},
   "outputs": [],
   "source": [
    "# spark infers the schema from the data but sometimes the inferred datatype is not correct.\n",
    "#so we need to define our own schema to be able to work properly\n",
    "# done with StructType\n",
    "# and can be specified while reading the data from the file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "30c5d8bf-894d-42f9-9d43-9540af07e466",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_schema = [\n",
    "               StructField('_c0', IntegerType(), nullable=True),\n",
    "               StructField('symbol', IntegerType(), nullable=True), \n",
    "               StructField('data', DateType(), True),\n",
    "               StructField('open', DoubleType(), True),\n",
    "               StructField('high', DoubleType(), True),\n",
    "               StructField('low', DoubleType(), True),\n",
    "               StructField('close', DoubleType(), True),\n",
    "               StructField('volume', IntegerType(), True),\n",
    "               StructField('adjusted', DoubleType(), True),\n",
    "               StructField('market.cap', StringType(), True),\n",
    "               StructField('sector', StringType(), True),\n",
    "               StructField('industry', StringType(), True),\n",
    "               StructField('exchange', StringType(), True),\n",
    "\n",
    "\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3e21f9eb-9fe5-492b-9a6d-02a32398c43f",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_schema = StructType(fields = data_schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "b6fe76f0-2758-4d48-a8dc-4cc81b136e66",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.csv('./data/stocks_price_final.csv', header=True, sep=',' , schema=final_schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3c90c156-1082-4d24-8c9b-cbe72e804f5e",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [1]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mdf\u001b[49m\u001b[38;5;241m.\u001b[39mprintSchema()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa9e74a1-92c7-4b96-b6e1-479885482ef5",
   "metadata": {},
   "source": [
    "### methods to inspect data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "7a950c7c-1a5f-4eff-a24a-8f3b62e4e25e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StructType(List(StructField(_c0,IntegerType,true),StructField(symbol,IntegerType,true),StructField(data,DateType,true),StructField(open,DoubleType,true),StructField(high,DoubleType,true),StructField(low,DoubleType,true),StructField(close,DoubleType,true),StructField(volume,IntegerType,true),StructField(adjusted,DoubleType,true),StructField(market.cap,StringType,true),StructField(sector,StringType,true),StructField(industry,StringType,true),StructField(exchange,StringType,true)))"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "8197e0ab-68f3-46ce-97d8-d8454a2fdc4e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('_c0', 'int'),\n",
       " ('symbol', 'int'),\n",
       " ('data', 'date'),\n",
       " ('open', 'double'),\n",
       " ('high', 'double'),\n",
       " ('low', 'double'),\n",
       " ('close', 'double'),\n",
       " ('volume', 'int'),\n",
       " ('adjusted', 'double'),\n",
       " ('market.cap', 'string'),\n",
       " ('sector', 'string'),\n",
       " ('industry', 'string'),\n",
       " ('exchange', 'string')]"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "45686cb0-9ed7-44eb-bb9b-06bc0f5d9945",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+----------+---------+---------+---------+---------+-------+---------+----------+-------------+--------------------+--------+\n",
      "|_c0|symbol|      data|     open|     high|      low|    close| volume| adjusted|market.cap|       sector|            industry|exchange|\n",
      "+---+------+----------+---------+---------+---------+---------+-------+---------+----------+-------------+--------------------+--------+\n",
      "|  1|  null|2019-09-12|     54.0|     58.0|     51.0|    52.75|7326300|    52.75|    $9.31B|Capital Goods|Biotechnology: La...|  NASDAQ|\n",
      "|  2|  null|2019-09-13|    52.75|   54.355|49.150002|    52.27|1025200|    52.27|    $9.31B|Capital Goods|Biotechnology: La...|  NASDAQ|\n",
      "|  3|  null|2019-09-16|52.450001|     56.0|52.009998|55.200001| 269900|55.200001|    $9.31B|Capital Goods|Biotechnology: La...|  NASDAQ|\n",
      "|  4|  null|2019-09-17|56.209999|60.900002|   55.423|56.779999| 602800|56.779999|    $9.31B|Capital Goods|Biotechnology: La...|  NASDAQ|\n",
      "|  5|  null|2019-09-18|56.849998|    62.27|55.650002|     62.0|1589600|     62.0|    $9.31B|Capital Goods|Biotechnology: La...|  NASDAQ|\n",
      "+---+------+----------+---------+---------+---------+---------+-------+---------+----------+-------------+--------------------+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/01/24 16:00:40 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: , symbol, date, open, high, low, close, volume, adjusted, market.cap, sector, industry, exchange\n",
      " Schema: _c0, symbol, data, open, high, low, close, volume, adjusted, market.cap, sector, industry, exchange\n",
      "Expected: _c0 but found: \n",
      "CSV file: file:///Users/ayusman/migrate/hadoop/Spark/data/stocks_price_final.csv\n"
     ]
    }
   ],
   "source": [
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "73fac1bf-adcf-4410-8002-a463e10405f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/01/24 16:00:40 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: , symbol, open, high, low, close, volume, adjusted, market.cap, sector, industry, exchange\n",
      " Schema: _c0, symbol, open, high, low, close, volume, adjusted, market.cap, sector, industry, exchange\n",
      "Expected: _c0 but found: \n",
      "CSV file: file:///Users/ayusman/migrate/hadoop/Spark/data/stocks_price_final.csv\n",
      "[Stage 23:=============================>                            (1 + 1) / 2]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------------+------+------------------+------------------+------------------+------------------+------------------+------------------+----------+----------------+--------------------+--------+\n",
      "|summary|              _c0|symbol|              open|              high|               low|             close|            volume|          adjusted|market.cap|          sector|            industry|exchange|\n",
      "+-------+-----------------+------+------------------+------------------+------------------+------------------+------------------+------------------+----------+----------------+--------------------+--------+\n",
      "|  count|          1729034|     0|           1726301|           1726301|           1726301|           1726301|           1725207|           1726301|   1729034|         1729034|             1729034| 1729034|\n",
      "|   mean|         864517.5|  null|15070.071703341051| 15555.06726813709|14557.808227578982|15032.714854330707|1397692.1627885813|  14926.1096887955|      null|            null|                null|    null|\n",
      "| stddev|499129.2670065541|  null|1111821.8002863196|1148247.1953514954|1072968.1558434265|1109755.9294000647| 5187522.908169119|1101877.6328940107|      null|            null|                null|    null|\n",
      "|    min|                1|  null|             0.072|             0.078|             0.052|             0.071|                 0|         -1.230099|    $1.01B|Basic Industries|Accident &Health ...|  NASDAQ|\n",
      "|    max|          1729034|  null|      1.60168176E8|      1.61601456E8|      1.55151728E8|      1.58376592E8|         656504200|      1.57249392E8|       $9B|  Transportation|Wholesale Distrib...|    NYSE|\n",
      "+-------+-----------------+------+------------------+------------------+------------------+------------------+------------------+------------------+----------+----------------+--------------------+--------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df.describe().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87e16ff5-bb31-436c-82cb-267a235e7eb6",
   "metadata": {},
   "source": [
    "### Column manipulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "89681991-4227-41a7-b374-d042ce82079a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating column\n",
    "df = df.withColumn('date', df.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "3898e41f-86f5-47ea-89ab-e9acf1f402cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "cbb5be75-1c58-4d2b-a013-67aee22b74ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "#rename column\n",
    "df = df.withColumnRenamed('date', 'date_changed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "0179d38d-b652-466e-8f8f-061c23e3dc04",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "84519e76-7e66-48fa-82ee-fc1df06b343c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#delete column\n",
    "df = df.drop('date_changed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "335230a0-3867-4f48-973d-eaa988eec81f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ffd31be-6831-4a11-8fc3-33d2b888e57c",
   "metadata": {},
   "source": [
    "### Dealing with missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "c2eb16c8-eec6-43cf-857c-f740ee3e7a91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4 ways to deal with them\n",
    "#1. dropping the row\n",
    "#2. fill with mean\n",
    "#3. fill with most frequent\n",
    "#4. fill using KNN (k Nearest Neighbour)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "aeb11468-06a9-47f3-951d-7669ecadb131",
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "Cannot resolve column name \"market.cap\" among (_c0, symbol, data, open, high, low, close, volume, adjusted, market.cap, sector, industry, exchange); did you mean to quote the `market.cap` column?",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [80]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mna\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdrop\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/migrate/hadoop/myvenv/lib/python3.9/site-packages/pyspark/sql/dataframe.py:2784\u001b[0m, in \u001b[0;36mDataFrameNaFunctions.drop\u001b[0;34m(self, how, thresh, subset)\u001b[0m\n\u001b[1;32m   2783\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdrop\u001b[39m(\u001b[38;5;28mself\u001b[39m, how\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124many\u001b[39m\u001b[38;5;124m'\u001b[39m, thresh\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, subset\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m-> 2784\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdropna\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhow\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhow\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mthresh\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mthresh\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msubset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msubset\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/migrate/hadoop/myvenv/lib/python3.9/site-packages/pyspark/sql/dataframe.py:2035\u001b[0m, in \u001b[0;36mDataFrame.dropna\u001b[0;34m(self, how, thresh, subset)\u001b[0m\n\u001b[1;32m   2032\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m thresh \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   2033\u001b[0m     thresh \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(subset) \u001b[38;5;28;01mif\u001b[39;00m how \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124many\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m-> 2035\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m DataFrame(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mna\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdrop\u001b[49m\u001b[43m(\u001b[49m\u001b[43mthresh\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jseq\u001b[49m\u001b[43m(\u001b[49m\u001b[43msubset\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msql_ctx)\n",
      "File \u001b[0;32m~/migrate/hadoop/myvenv/lib/python3.9/site-packages/py4j/java_gateway.py:1309\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1303\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1304\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1305\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1306\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1308\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1309\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1310\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1312\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1313\u001b[0m     temp_arg\u001b[38;5;241m.\u001b[39m_detach()\n",
      "File \u001b[0;32m~/migrate/hadoop/myvenv/lib/python3.9/site-packages/pyspark/sql/utils.py:117\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    113\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    116\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> 117\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[1;32m    118\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    119\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: Cannot resolve column name \"market.cap\" among (_c0, symbol, data, open, high, low, close, volume, adjusted, market.cap, sector, industry, exchange); did you mean to quote the `market.cap` column?"
     ]
    }
   ],
   "source": [
    "df.na.drop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a66f9cf-ea08-4dcc-b3c6-f19e4dfb037b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
