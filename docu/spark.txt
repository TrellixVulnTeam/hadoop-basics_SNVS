Spark Architecture
Driver Program - Driver executes the main program of Spark application and creates SparkContext that coordinates the execution of jobs.
Driver program assigns jobs to executors which are processes running on worker nodes of the cluster.
YARN or Mesos which are cluster manager are responsible for allocation of physical resources to Spark Applications.


Entry Points - communicate with the data sources
Every Spark application needs an entry point that allows it to communicate with data sources and perform certain operations sucs as reading and writing data.

-SparkContext
establish communication with the cluster and resource manager inorder to execute jobs.
it also enables access to other 2 contexts

-SQLContext --> for structured data processing
entry point to SparkSQL which is a Spark module for structured data processing.
Once SqlContext is initialized, it can be used to perform sql-like operations over datasets and dataframes.

-HiveContext
for communicating with hive.

-SparkSession
Spark 2.0 and above supports SparkSession which is an entry point  and replaces SQLContext and HiveContext. 






Spark Context 
-it is the internal engine that allows the connections with the clusters. If you want to run an operation, you need a SparkContext.

import SparkContext from pyspark
sc = SparkContext()

- After getting SparkContext ready, we can create RDDs for cluster computing.
nums = sc.parallelize([1,2,3,4])

We can access rows with take
nums.take(1). --> access first row

